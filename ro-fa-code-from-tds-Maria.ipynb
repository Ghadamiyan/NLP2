{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:52:50.123107Z",
     "iopub.status.busy": "2022-01-15T22:52:50.122395Z",
     "iopub.status.idle": "2022-01-15T22:52:50.153995Z",
     "shell.execute_reply": "2022-01-15T22:52:50.153364Z",
     "shell.execute_reply.started": "2022-01-15T22:52:50.123002Z"
    },
    "id": "tmYJBBbwyiX3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:52:50.155414Z",
     "iopub.status.busy": "2022-01-15T22:52:50.155224Z",
     "iopub.status.idle": "2022-01-15T22:52:59.264531Z",
     "shell.execute_reply": "2022-01-15T22:52:59.263746Z",
     "shell.execute_reply.started": "2022-01-15T22:52:50.155391Z"
    },
    "id": "SqADZ2Sgx5Ua",
    "outputId": "16d10e79-af3f-4a82-d9e6-9843292775aa"
   },
   "outputs": [],
   "source": [
    "! pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:52:59.267601Z",
     "iopub.status.busy": "2022-01-15T22:52:59.267334Z",
     "iopub.status.idle": "2022-01-15T22:53:00.745556Z",
     "shell.execute_reply": "2022-01-15T22:53:00.744780Z",
     "shell.execute_reply.started": "2022-01-15T22:52:59.267565Z"
    },
    "id": "5xKFwtNEx5Uc",
    "outputId": "4a9ced50-0274-4031-9978-b91bfbc7d0a0"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:00.747727Z",
     "iopub.status.busy": "2022-01-15T22:53:00.747515Z",
     "iopub.status.idle": "2022-01-15T22:53:00.751701Z",
     "shell.execute_reply": "2022-01-15T22:53:00.750914Z",
     "shell.execute_reply.started": "2022-01-15T22:53:00.747700Z"
    },
    "id": "MX7DsEmOx5Ud"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-mul-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:00.753458Z",
     "iopub.status.busy": "2022-01-15T22:53:00.753008Z",
     "iopub.status.idle": "2022-01-15T22:53:02.085347Z",
     "shell.execute_reply": "2022-01-15T22:53:02.081964Z",
     "shell.execute_reply.started": "2022-01-15T22:53:00.753419Z"
    },
    "id": "biPo8vFTx5Ue",
    "outputId": "2b9ffb11-dca6-4dc7-ecf5-9d6856a9281b"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:02.086745Z",
     "iopub.status.busy": "2022-01-15T22:53:02.086497Z",
     "iopub.status.idle": "2022-01-15T22:53:13.700978Z",
     "shell.execute_reply": "2022-01-15T22:53:13.700254Z",
     "shell.execute_reply.started": "2022-01-15T22:53:02.086713Z"
    },
    "id": "agDlgrOix5Uj",
    "outputId": "ff092d00-3d4d-4a6f-bca5-f33a49a64c7d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:13.702690Z",
     "iopub.status.busy": "2022-01-15T22:53:13.702454Z",
     "iopub.status.idle": "2022-01-15T22:53:13.708915Z",
     "shell.execute_reply": "2022-01-15T22:53:13.707049Z",
     "shell.execute_reply.started": "2022-01-15T22:53:13.702656Z"
    },
    "id": "qvYg6BXJx5Ul",
    "outputId": "20923805-e42b-4099-a2c8-d645393029d4"
   },
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:50.754000Z",
     "iopub.status.busy": "2022-01-15T22:53:50.753741Z",
     "iopub.status.idle": "2022-01-15T22:53:50.757480Z",
     "shell.execute_reply": "2022-01-15T22:53:50.756796Z",
     "shell.execute_reply.started": "2022-01-15T22:53:50.753971Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:51.094841Z",
     "iopub.status.busy": "2022-01-15T22:53:51.094263Z",
     "iopub.status.idle": "2022-01-15T22:53:56.221524Z",
     "shell.execute_reply": "2022-01-15T22:53:56.220788Z",
     "shell.execute_reply.started": "2022-01-15T22:53:51.094805Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/ro-fa-clean-translate/train_df.csv')\n",
    "test_df = pd.read_csv('../input/ro-fa-clean-translate/test_df.csv')\n",
    "train_df['ro'] = train_df.ro.astype(str).apply(lambda x: x.replace('</i>', ''))\n",
    "test_df['ro'] = test_df.ro.astype(str).apply(lambda x: x.replace('</i>', ''))\n",
    "train_df['fa'] = train_df.fa.astype(str).apply(lambda x: x.replace('</i>', ''))\n",
    "test_df['fa'] = test_df.fa.astype(str).apply(lambda x: x.replace('</i>', ''))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(train_df, shuffle=True, test_size=len(train_df)//20, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, dataset_dict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "raw_datasets = dataset_dict.DatasetDict({'train':train_dataset, 'validation':val_dataset, 'test':test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:56.223440Z",
     "iopub.status.busy": "2022-01-15T22:53:56.223193Z",
     "iopub.status.idle": "2022-01-15T22:53:56.228982Z",
     "shell.execute_reply": "2022-01-15T22:53:56.228259Z",
     "shell.execute_reply.started": "2022-01-15T22:53:56.223406Z"
    },
    "id": "e1fnOpO6x5Um"
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"fa\"\n",
    "target_lang = \"ro\"\n",
    "def preprocess_function(dataset):\n",
    "    inputs = dataset[source_lang]\n",
    "    targets = dataset[target_lang]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:56.231027Z",
     "iopub.status.busy": "2022-01-15T22:53:56.230527Z",
     "iopub.status.idle": "2022-01-15T22:53:56.249730Z",
     "shell.execute_reply": "2022-01-15T22:53:56.248996Z",
     "shell.execute_reply.started": "2022-01-15T22:53:56.230990Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_function(train_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:53:58.667106Z",
     "iopub.status.busy": "2022-01-15T22:53:58.666835Z",
     "iopub.status.idle": "2022-01-15T22:56:22.656739Z",
     "shell.execute_reply": "2022-01-15T22:56:22.655952Z",
     "shell.execute_reply.started": "2022-01-15T22:53:58.667062Z"
    },
    "id": "uZlsJFZnx5Uo",
    "outputId": "ffb2f5f3-1127-4c74-8094-1605e531ead4"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:56:22.659028Z",
     "iopub.status.busy": "2022-01-15T22:56:22.658766Z",
     "iopub.status.idle": "2022-01-15T22:56:22.664458Z",
     "shell.execute_reply": "2022-01-15T22:56:22.663726Z",
     "shell.execute_reply.started": "2022-01-15T22:56:22.658990Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:56:22.666335Z",
     "iopub.status.busy": "2022-01-15T22:56:22.665850Z",
     "iopub.status.idle": "2022-01-15T22:56:41.328278Z",
     "shell.execute_reply": "2022-01-15T22:56:41.327524Z",
     "shell.execute_reply.started": "2022-01-15T22:56:22.666297Z"
    },
    "id": "IvrIjd-Lx5Uq",
    "outputId": "590f26cf-1df0-44d4-f5b2-106cad098bc4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:56:41.330478Z",
     "iopub.status.busy": "2022-01-15T22:56:41.330278Z",
     "iopub.status.idle": "2022-01-15T22:56:41.380693Z",
     "shell.execute_reply": "2022-01-15T22:56:41.380047Z",
     "shell.execute_reply.started": "2022-01-15T22:56:41.330453Z"
    },
    "id": "qk2p1KEwx5Ur",
    "outputId": "29763c0e-e709-4219-f2c5-98c6dfd096b1"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:56:41.382059Z",
     "iopub.status.busy": "2022-01-15T22:56:41.381752Z",
     "iopub.status.idle": "2022-01-15T22:56:41.388607Z",
     "shell.execute_reply": "2022-01-15T22:56:41.387900Z",
     "shell.execute_reply.started": "2022-01-15T22:56:41.382021Z"
    },
    "id": "qV9wfuvZx5Us"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:56:41.393195Z",
     "iopub.status.busy": "2022-01-15T22:56:41.390928Z",
     "iopub.status.idle": "2022-01-15T22:56:41.407053Z",
     "shell.execute_reply": "2022-01-15T22:56:41.406282Z",
     "shell.execute_reply.started": "2022-01-15T22:56:41.393167Z"
    },
    "id": "mHqp_FFhx5Uu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T22:56:41.408652Z",
     "iopub.status.busy": "2022-01-15T22:56:41.408214Z"
    },
    "id": "c7zuF8rLx5Uy"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sonIRAfDx5Uz",
    "outputId": "b1e5fd5d-28c1-4c35-ccc2-da386f37ff47"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-SSyPwox5U1",
    "outputId": "c06f1ef4-8a5e-49a7-ef48-3c4997d3623f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('opus-mt-en-ro-finetuned-en-to-ro'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
